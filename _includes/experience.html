<h2 style="padding-top: 10px;"><b>experience & volunteering</b></h2>
<p>For more past work experience, please visit my <a href="https://www.linkedin.com/in/keyuw/">LinkedIn</a></b></p>
<table style="padding-top: 10px; width:100%;border:0px;border-spacing:0px;border-collapse:separate;"><tbody>  
  <tr>
    <td style="padding-right: 20px; width:25%;vertical-align:middle">
      <img src="assets/img/selfsup.jpg" alt="clean-usnob" width="160" height="140">
    </td>
    <td width="75%" valign="middle">
      <p1>
      <strong>California Gold Rush: </strong>
      <br> [Oral Presentation @ GIS Day 2022] @ <strong><a href="https://engsci.utoronto.ca/program/thesis/">U of T Eng Sci</a></strong>, supervised by <strong><a href="https://www.trailab.utias.utoronto.ca/">Sanja Fidler</a></strong> 
      <br> <br> Conducting an analysis of inter-image relationships for contrastive learning to investigate how context between images can be used to improve part-based dense representation learning. </p1>
      <br> <br>
    </td>
  </tr>
</tbody></table>

<table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
  <tr>
    <td style="padding-right: 20px; width:25%;vertical-align:middle">
      <img src="assets/img/carla_imi.gif" alt="clean-usnob" width="160" height="140">
    </td>
    <td width="75%" valign="middle">
      <p1>
      <strong>Unsupervised Multimodal Representation Learning for Robot Controls</strong>
      <br> [Research Project] @ <strong><a href="https://rvl.cs.toronto.edu/">RVL</a></strong>. With Yewon L., Pranit C.
      <br> <br> Investigated how unsupervised representation learning could be utilized in an imitation learning (IL) task such as robot path following. In this project, we successfully trained a multimodal representation from unlabelled path traversal data, fine-tuned on minimal labeled data.</p1>
    </td>
  </tr>
</tbody></table>

<table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
  <tr>
    <td style="padding-right: 20px; width:25%;vertical-align:middle">
      <img src="assets/img/multiview.JPG" alt="clean-usnob" width="160" height="140">
    </td>
    <td width="75%" valign="middle">
      <p1>
      <strong>Analysis of Monocular 3D Object Detection Network Performance on Multiview Datasets</strong>
      <br> [Research Presentation] @ <strong><a href="https://www.trailab.utias.utoronto.ca/">TRAILab</a></strong>, funded by <strong>NSERC USRA</strong> 
      <br> <a href="https://github.com/JuliaChae/M3D-RPN-Waymo">Code</a> 
      <br> <br> Designed and executed an analysis of effect of camera perspectives on object detection networks, identifying limitations of multi-view self-driving datasets and current state-of-art monocular networks. The performance of trained monocular networks on various views of multiview dataset was analyzed with respect to multiple factors including distance, rotation and level of occlusion.</p1>
    </td>
  </tr>
</tbody></table>

<table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
  <tr>
    <td style="padding-right: 20px; width:25%;vertical-align:middle">
      <img src="assets/img/pointfusion.png" alt="clean-usnob" width="160" height="140">
    </td>
    <td width="75%" valign="middle">
      <p1>
      <strong>Adversarial Examples for Multimodal Object Detection Networks </strong>
      <br> [Research Project] @ <strong><a href="https://rvl.cs.toronto.edu/">RVL</a></strong>, funded by <strong>ESROP Dr Allison Mackay Award</strong>
      <br> <a href="https://github.com/JuliaChae/Pointfusion">Code</a>     
      <br> <br> Led the implementation of multi-sensor object detection neural networks to state-of-art performance using PyTorch for an adversarial examples project, to improve robustness of image and lidar perception models. Trained fasterRCNN on nuScenes self driving car dataset and implemented Pointfusion architecture based on the original paper.</p1>
    </td>
  </tr>
</tbody></table>

<table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
  <tr>
    <td style="padding-right: 20px; width:25%;vertical-align:middle">
      <img src="assets/img/RSX.gif" alt="clean-usnob" width="160" height="140">
    </td>
    <td width="75%" valign="middle">
    <p1>
      <strong>Autonomous Rover Drive System</strong>
      <br> [Competition] University/European/Canadian Rover Challenges 
      <br> <a href="https://github.com/rsx-utoronto/rover/tree/develop">Code</a> 
      <br> <br> Spearheaded development of rover drive control software pipeline which includes joystick integration, configuration of I2C protocol for motor drivers, variable speed control and AR tag detection. Languages used were Python, C++ and ROS was utilized to handle communication between topics.</p1>
    </td>
  </tr>
</tbody></table>
<table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
  <tr>
    <td style="padding-right: 20px; width:25%;vertical-align:middle">
      <img src="assets/img/rob301.jpg" alt="clean-usnob" width="160" height="140">
    </td>
    <td width="75%" valign="middle">
    <p1>
      <strong>Robot Mail Delivery Localization System</strong>
      <br>[Final Project], University of Toronto, ROB301 Introduction to Robotics. With Samantha U.
      <br> <a href="assets/pdf/ROB301_FinalReport.pdf">Detailed Report</a> / Code Coming Soon 
      <br> <br> A PID control system, bayesian localization and state measurement models were developed in order to enable the robot to navigate an unknown hallway to deliver mail at specified destinations. The project was completed virtually using the Turtlebot 3 Waffle Pi robot simulated in a Gazebo environment. </p1>
    </td>
  </tr>
</tbody></table>
<table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
  <tr>
    <td style="padding-right: 20px; width:25%;vertical-align:middle">
      <img src="assets/img/aps360.jpg" alt="clean-usnob" width="160" height="140">
    </td>
    <td width="75%" valign="middle">
      <p1>
      <strong>Accent Classification Network</strong>
      <br>[Final Project], University of Toronto, APS360 Applied Fundamentals of Machine Learning. With Mingshi C., Catherine G., Rocco R.
      <br> <a href="assets/pdf/APS360_FinalReport.pdf">Detailed Report</a> / Code Coming Soon 
      <br> <br> Motivated by challenges experienced with voice-controlled devices, our team developed a neural network speech accent classifier which takes an accented English phrase as input and correctly identifies the origin of that speakerâ€™s accent. The network developed was a CRNN architecture with mixed convolutional and recurrent neural network layers which learned both temporal and characteristic features of the audio data. The audio was processed into Mel-frequency cepstral coefficients (MFCC) features for input. </p1>
    </td>
  </tr>
</tbody></table>
<table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
  <tr>
    <td style="padding-right: 20px; width:25%;vertical-align:middle">
      <img src="assets/img/ESC301.jpg" alt="clean-usnob" width="160" height="140">
    </td>
    <td width="75%" valign="middle">
      <p1>
      <strong>Autonomous Electric Car Charger Robot </strong>
      <br>[Final Project], University of Toronto, ESC301 Praxis III. With Daniel R. and Chan Y.
      <br> <a href="assets/pdf/ESC301_Project Proposal Report.pdf">Detailed Design Report</a> 
      <br> <br> Designed, conceptualized and prototyped an autonomous robot which locates and traverses to a charging port on a car to plug in the charger, utilizing time-of-flight sensor, color sensor, and pi-camera. Developed a modular program in Python and C++, each responsible for sensor or chassis control; utilized embedded programing</p1>
    </td>
  </tr>
</tbody></table>
<table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
  <tr>
    <td style="padding-right: 20px; width:25%;vertical-align:middle">
      <img src="assets/img/Praxis2.JPG" alt="clean-usnob" width="160" height="140">
    </td>
    <td width="75%" valign="middle">
      <p1>
      <strong>Sourdough IR Fermentometer Autonomous Robot </strong>
      <br> [Final Project], University of Toronto, ESC102 Praxis II. With Mingshi C., Daniela L., Peter S.
      <br> <a href="assets/pdf/ShowcasePoster.pdf">Poster</a> / <a href="assets/img/praxis2_demo.mp4">Demo Video</a> 
      <br> <br> Designed a fully automated robotics system to inform sourdough bakers when their dough is ready to be put into the oven, in collaboration with local Toronto bakers at Forno Cultura and Blackbird Bakeries. The system develops a custom yeast-growth model and while the dough is expanding, the robot uses a 2 axis linear stage, time of flight sensor and temperature sensor to track the dough growth and alerts the bakers when it is ready. </p1>
    </td>
  </tr>
</tbody></table>
<table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
  <tr>
    <td style="padding-right: 20px; width:25%;vertical-align:middle">
      <img src="assets/img/Chess.png" alt="clean-usnob" width="160" height="140">
    </td>
    <td width="75%" valign="middle">
      <p1>
      <strong>Chess AI</strong>
      <br> [Class Project], University of Toronto, CSC190 Algorithms and Data Structures.
      <br> <a href="https://github.com/JuliaChae/Chess-AI">Code</a>
      <br> <br>Developed an automatic chess player that while utilizes alpha-beta pruning on a min-max game tree in order to select the best move against a human opponent. </p1>
    </td>
  </tr>
</tbody></table>

</tbody></table>
<table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  
  <tr>
    <td style="padding-right: 20px; width:25%;vertical-align:middle">
      <img src="assets/img/Gameboy.gif" alt="clean-usnob" width="160" height="140">
    </td>
    <td width="75%" valign="middle">
      <p1>
      <strong>Original Gameboy Project: Bit's Adventures</strong>
      <br> [Final Project], University of Toronto, MIE438 Microcontrollers and Embedded Processors.
      <br> <a href="https://github.com/Maelstrum127/MIE438-GameBoy-Project">Code</a> / <a href="https://www.youtube.com/watch?v=2vMHTOsuGYA&ab_channel=CatGlossop">Demo Video</a>
      <br> <br> Bitâ€™s Adventure is an original interactive RPG for the Original Game Boy which was coded using gbdk package in C. The game can be played on Game Boy emulators such as mGBA. </p1>
    </td>
  </tr>
</tbody></table>